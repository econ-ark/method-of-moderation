# Method of Moderation Benchmarking Guide

## Overview

The Method of Moderation benchmarking system follows industry-standard practices for capturing and reporting reproduction run times across different hardware, operating systems, and configurations.

## Quick Start

### Running a Benchmark

```bash
# Benchmark minimal reproduction (~5 minutes)
./reproduce/benchmarks/benchmark.sh --min

# Benchmark full reproduction (all tests, paper, notebooks)
./reproduce/benchmarks/benchmark.sh

# With notes
./reproduce/benchmarks/benchmark.sh --min --notes "Testing M1 Max performance"
```

### Viewing Results

```bash
# View latest benchmark
cat reproduce/benchmarks/results/latest.json | jq .

# View system info from latest
cat reproduce/benchmarks/results/latest.json | jq '.system'

# Check duration
cat reproduce/benchmarks/results/latest.json | jq '.duration_seconds'
```

## What Gets Captured

### System Information

- **OS**: Operating system name, version, kernel
- **CPU**: Model, architecture (x86_64/arm64), core count, frequency
- **Memory**: Total RAM, available RAM
- **Disk**: Type (SSD/HDD/NVMe), free space
- **Hostname**: Machine name (can be anonymized)

### Environment

- **Python Version**: e.g., 3.12.0
- **Environment Type**: uv, conda, venv
- **Virtual Environment Path**: Location of .venv or conda env
- **Key Packages**: Versions of econ-ark, mystmd, numpy, scipy, matplotlib, numba, etc.

### Git State

- **Commit**: Git commit hash
- **Branch**: Active branch name
- **Dirty**: Whether there are uncommitted changes

### Timing

- **Start/End Timestamps**: ISO 8601 format (UTC)
- **Duration**: Total seconds + formatted time (HH:MM:SS)
- **Exit Status**: Success (0) or error code

### Metadata

- **User**: Username (for tracking who ran benchmark)
- **Session ID**: Unique process ID
- **CI Flag**: Whether run in CI/CD environment
- **Notes**: Optional user-provided notes

## Data Format

Benchmarks are stored as JSON following these industry standards:

- [pytest-benchmark](https://pytest-benchmark.readthedocs.io/) format
- [GitHub Actions benchmark](https://github.com/benchmark-action/github-action-benchmark) conventions
- Cross-platform compatibility (macOS, Linux, Windows/WSL2)

Example structure:

```json
{
  "benchmark_version": "1.0.0",
  "benchmark_id": "20250117-143022_min_darwin-arm64",
  "timestamp": "2025-01-17T14:30:22Z",
  "reproduction_mode": "min",
  "exit_status": 0,
  "duration_seconds": 245,
  "system": { ... },
  "environment": { ... },
  "git": { ... },
  "metadata": { ... }
}
```

## Storage Location

```
reproduce/benchmarks/
├── README.md                    # Overview and documentation
├── BENCHMARKING_GUIDE.md        # This file
├── schema.json                  # JSON schema for validation
├── benchmark.sh                 # Main benchmarking wrapper
├── capture_system_info.py       # System info capture utility
└── results/                     # Benchmark results (gitignored by default)
    ├── README.md
    ├── .gitignore
    ├── latest.json -> ...       # Symlink to latest
    ├── autogenerated/           # Auto-created benchmarks
    └── saved/                   # Reference benchmarks
```

## Privacy and Data Sharing

### What's Safe to Share

- OS, CPU, memory specs
- Duration and performance metrics
- Python and package versions
- Git commit (public repo)

### What to Redact

- Hostname (if it contains personal info)
- Username (if privacy is a concern)
- Virtual environment paths (may contain usernames)
- Any custom notes with sensitive info

### Sharing Benchmarks

```bash
# Create anonymized copy
jq 'del(.system.hostname, .metadata.user, .environment.virtual_env)' \
   results/benchmark.json > benchmark_anonymous.json

# Commit reference benchmark
git add -f reproduce/benchmarks/results/saved/20250117_reference_m1max.json
git commit -m "Add reference benchmark: M1 Max 2021"
```

## Use Cases

### 1. Performance Tracking
Track reproduction time over different code versions:

```bash
# Before optimization
./reproduce/benchmarks/benchmark.sh --min --notes "Before optimization"

# After optimization
./reproduce/benchmarks/benchmark.sh --min --notes "After optimization"

# Compare
jq '.duration_seconds' results/autogenerated/*.json
```

### 2. Hardware Comparison
Compare performance across different machines:

```bash
# On laptop
./reproduce/benchmarks/benchmark.sh --min --notes "MacBook Pro M1 2021"

# On desktop
./reproduce/benchmarks/benchmark.sh --min --notes "AMD Ryzen 9 5900X"
```

### 3. Environment Comparison
Compare UV vs Conda:

```bash
# With UV
source .venv-linux-aarch64/bin/activate
./reproduce/benchmarks/benchmark.sh --min --notes "UV environment"

# With Conda
conda activate moderation
./reproduce/benchmarks/benchmark.sh --min --notes "Conda environment"
```

### 4. CI/CD Integration
Automated performance regression detection:

```yaml
- name: Run Benchmark
  run: ./reproduce/benchmarks/benchmark.sh --min
  
- name: Upload Results
  uses: actions/upload-artifact@v3
  with:
    name: benchmark-results
    path: reproduce/benchmarks/results/latest.json
```

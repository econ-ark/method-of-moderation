# Method of Moderation Reproduction Benchmarks

This directory contains benchmark data for reproduction runs, following industry-standard formats for performance tracking.

## Purpose

Track and document the time required to reproduce Method of Moderation results across different:

- Hardware configurations (CPU, RAM, storage)
- Operating systems (macOS, Linux, Windows/WSL2)
- Reproduction modes (`full` via `reproduce.sh`, `min` via `reproduce_min.sh`)
- Environment types (UV vs Conda)

## Quick Start

### Running a Benchmark

```bash
# Benchmark minimal reproduction (<5 minutes)
./reproduce/benchmarks/benchmark.sh --min

# Benchmark full reproduction (all tests, paper, notebooks)
./reproduce/benchmarks/benchmark.sh

# With notes
./reproduce/benchmarks/benchmark.sh --min --notes "Testing M1 Max performance"
```

### Viewing Results

```bash
# View latest benchmark
cat reproduce/benchmarks/results/latest.json | jq .

# View system info from latest
cat reproduce/benchmarks/results/latest.json | jq '.system'

# Check duration
cat reproduce/benchmarks/results/latest.json | jq '.duration_seconds'
```

## Benchmark Format

Benchmarks are stored in **JSON format** following the [pytest-benchmark](https://pytest-benchmark.readthedocs.io/) and [GitHub Actions benchmark](https://github.com/benchmark-action/github-action-benchmark) conventions.

Each benchmark includes:
- System information (OS, CPU, memory, disk)
- Python environment (version, packages, virtual environment)
- Git state (commit, branch, dirty flag)
- Timing information (start, end, duration)
- Reproduction mode (`full` or `min`)
- Exit status and metadata

## Directory Structure

```
reproduce/benchmarks/
├── README.md                    # This file
├── BENCHMARKING_GUIDE.md        # Detailed usage guide
├── schema.json                  # JSON schema for validation
├── benchmark.sh                 # Main benchmarking wrapper
├── capture_system_info.py       # System info capture utility
└── results/                     # Benchmark results (gitignored by default)
    ├── .gitignore
    ├── README.md
    ├── latest.json -> ...       # Symlink to latest benchmark
    ├── autogenerated/           # Auto-created benchmarks
    └── saved/                   # Manually saved reference benchmarks
```

## For More Information

See [BENCHMARKING_GUIDE.md](./BENCHMARKING_GUIDE.md) for:
- Detailed usage instructions
- Examples for different use cases
- Privacy considerations
- How to share benchmarks
